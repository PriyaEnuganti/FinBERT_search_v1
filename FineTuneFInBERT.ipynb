{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv-GuG4YULhS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the pre-trained FinBERT model and tokenizer\n",
        "model = BertModel.from_pretrained('ProsusAI/finbert')\n",
        "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "\n",
        "# Load your query and document data into lists\n",
        "queries = ['query 1', 'query 2', ...]\n",
        "documents = ['document 1', 'document 2', ...]\n",
        "\n",
        "# Tokenize the query and document data\n",
        "query_tokens = tokenizer(queries, padding=True, truncation=True, return_tensors='pt')\n",
        "document_tokens = tokenizer(documents, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Convert the tokens to tensors and move them to the device\n",
        "query_input_ids = query_tokens['input_ids'].to(device)\n",
        "query_attention_mask = query_tokens['attention_mask'].to(device)\n",
        "document_input_ids = document_tokens['input_ids'].to(device)\n",
        "document_attention_mask = document_tokens['attention_mask'].to(device)\n",
        "\n",
        "# Define the two-tower model\n",
        "class TwoTowerModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TwoTowerModel, self).__init__()\n",
        "        self.bert_query = BertModel.from_pretrained('ProsusAI/finbert')\n",
        "        self.bert_document = BertModel.from_pretrained('ProsusAI/finbert')\n",
        "        \n",
        "    def forward(self, query_input_ids, query_attention_mask, document_input_ids, document_attention_mask):\n",
        "        query_embeddings = self.bert_query(query_input_ids, attention_mask=query_attention_mask)[0][:, 0, :]\n",
        "        document_embeddings = self.bert_document(document_input_ids, attention_mask=document_attention_mask)[0][:, 0, :]\n",
        "        return query_embeddings, document_embeddings\n",
        "\n",
        "# Initialize the two-tower model and move it to the device\n",
        "model = TwoTowerModel().to(device)\n",
        "\n",
        "# Define the optimizer and the learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "num_epochs = 3\n",
        "num_warmup_steps = 100\n",
        "num_training_steps = num_epochs * len(queries)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
        "\n",
        "# Train the two-tower model\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(len(queries)):\n",
        "        # Forward pass\n",
        "        query_embeddings, document_embeddings = model(query_input_ids[i:i+1], query_attention_mask[i:i+1], \n",
        "                                                      document_input_ids, document_attention_mask)\n",
        "        # Compute the dot product similarity scores\n",
        "        similarity_scores = torch.mm(query_embeddings, document_embeddings.t())\n",
        "        \n",
        "        # Compute the loss\n",
        "        loss = torch.nn.functional.cross_entropy(similarity_scores, torch.tensor([i]).to(device))\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Save the model and the tokenizer\n",
        "model.save_pretrained('finetuned_finbert')\n",
        "tokenizer.save_pretrained('finetuned_finbert')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7_8z_NySVGuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8tY8v1YaVG4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load the trained FinBERT model and tokenizer\n",
        "model_name = \"ProsusAI/finbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Load the dataset that you want to index and retrieve documents from\n",
        "dataset = [...]  # Your dataset goes here\n",
        "\n",
        "# Tokenize and encode the documents in the dataset using the trained tokenizer to get the embeddings for each document\n",
        "document_embeddings = []\n",
        "for doc in dataset:\n",
        "    inputs = tokenizer(doc, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embeddings = outputs.pooler_output\n",
        "        document_embeddings.append(embeddings.numpy()[0])\n",
        "\n",
        "# Create an Annoy index and add the embeddings of all the documents in the dataset to it\n",
        "from annoy import AnnoyIndex\n",
        "embedding_size = document_embeddings[0].shape[0]\n",
        "index = AnnoyIndex(embedding_size, metric='euclidean')\n",
        "for i, emb in enumerate(document_embeddings):\n",
        "    index.add_item(i, emb)\n",
        "index.build(10)\n",
        "\n",
        "# For each user query, encode it using the same tokenizer and pass it through the query tower of the two-tower model to get the query embedding\n",
        "query = \"How do I invest in stocks?\"\n",
        "query_inputs = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    query_embedding = model(**query_inputs).pooler_output.numpy()[0]\n",
        "\n",
        "# Use the Annoy index to retrieve the nearest neighbors to the query embedding\n",
        "n_neighbors = 5\n",
        "neighbors = index.get_nns_by_vector(query_embedding, n_neighbors)\n",
        "\n",
        "# Return the relevant documents to the user\n",
        "relevant_documents = [dataset[i] for i in neighbors]\n",
        "print(relevant_documents)"
      ],
      "metadata": {
        "id": "RSIRocTzVG6z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}