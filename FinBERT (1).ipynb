{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "372GeJfG1fGt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "# Define the hyperparameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "embedding_size = 768\n",
        "num_trees = 10\n",
        "num_nearest_neighbors = 10\n",
        "\n",
        "# Load the data\n",
        "with open('data.txt', 'r') as f:\n",
        "    data = [line.strip().split('\\t') for line in f.readlines()]\n",
        "\n",
        "queries = [d[0] for d in data]\n",
        "documents = [d[1] for d in data]\n",
        "urls = [d[2] for d in data]\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "model = AutoModel.from_pretrained('ProsusAI/finbert')\n",
        "\n",
        "# Define the query tower\n",
        "class QueryTower(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QueryTower, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooler_output = outputs.pooler_output\n",
        "        return pooler_output\n",
        "\n",
        "# Define the document tower\n",
        "class DocumentTower(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DocumentTower, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooler_output = outputs.pooler_output\n",
        "        return pooler_output\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = torch.nn.CosineEmbeddingLoss()\n",
        "\n",
        "# Define the optimizer for the query tower\n",
        "optimizer_query = torch.optim.Adam(QueryTower().parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the query tower\n",
        "query_tower = QueryTower()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(len(queries)):\n",
        "        # Encode the query\n",
        "        query = tokenizer(queries[i], padding=True, truncation=True, return_tensors='pt')\n",
        "        # Forward pass\n",
        "        query_embedding = query_tower(query['input_ids'], query['attention_mask'])\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(query_embedding, torch.ones_like(query_embedding), torch.ones_like(query_embedding))\n",
        "        # Backward pass\n",
        "        optimizer_query.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_query.step()\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {loss.item():.4f}')\n",
        "\n",
        "# Define the optimizer for the document tower\n",
        "optimizer_document = torch.optim.Adam(DocumentTower().parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the document tower\n",
        "document_tower = DocumentTower()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(len(documents)):\n",
        "        # Encode the document\n",
        "        document = tokenizer(documents[i], padding=True, truncation=True, return_tensors='pt')\n",
        "        # Forward pass\n",
        "        document_embedding = document_tower(document['input_ids'], document['attention_mask'])\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(document_embedding, torch.zeros_like(document_embedding), torch.zeros_like(document_embedding))\n",
        "        # Backward pass\n",
        "        optimizer_document.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_document.step()\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {loss.item():.4f}')\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, key in enumerate(encoded_data.keys()):\n",
        "    index.add_item(i, encoded_data[key])\n",
        "\n",
        "# Build the Annoy index\n",
        "index.build(num_trees)\n",
        "\n",
        "# Save the Annoy index to disk\n",
        "index.save(index_file_path)\n",
        "\n",
        "# Query the Annoy index for the nearest neighbors\n",
        "for i, query in enumerate(queries):\n",
        "    # Encode the query\n",
        "    encoded_query = encode_text(query)\n",
        "    # Get the nearest neighbors\n",
        "    nearest_neighbors = index.get_nns_by_vector(encoded_query, num_nearest_neighbors)\n",
        "    # Print the results\n",
        "    print(f'Query {i}: {query}')\n",
        "    for neighbor in nearest_neighbors:\n",
        "        key = list(encoded_data.keys())[neighbor]\n",
        "        print(f'  Neighbor: {key}')\n",
        "        print(f'  Distance: {index.get_distance(i, neighbor)}')"
      ],
      "metadata": {
        "id": "cpR2OVZ_14eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(query, query_tower, document_tower, tokenizer, annoy_index, num_nearest_neighbors):\n",
        "    # Encode the query\n",
        "    query = tokenizer(query, padding=True, truncation=True, return_tensors='pt')\n",
        "    # Forward pass through the query tower\n",
        "    query_embedding = query_tower(query['input_ids'], query['attention_mask']).detach().numpy().squeeze()\n",
        "    # Find the nearest neighbors\n",
        "    nearest_neighbors = annoy_index.get_nns_by_vector(query_embedding, num_nearest_neighbors)\n",
        "    # Get the corresponding documents and URLs\n",
        "    documents = [data[i][1] for i in nearest_neighbors]\n",
        "    urls = [data[i][2] for i in nearest_neighbors]\n",
        "    return documents, urls"
      ],
      "metadata": {
        "id": "71cYyWFi4LOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iqFQWA540xlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Load the FinBERT model\n",
        "finbert_model = BertModel.from_pretrained('ProsusAI/finbert')\n",
        "\n",
        "# Define the Two-Tower model architecture\n",
        "class TwoTowerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TwoTowerModel, self).__init__()\n",
        "        self.query_encoder = finbert_model\n",
        "        self.doc_encoder = finbert_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc1 = nn.Linear(1536, 256)\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "    def forward(self, query_inputs, doc_inputs):\n",
        "        query_outputs = self.query_encoder(input_ids=query_inputs['input_ids'], attention_mask=query_inputs['attention_mask'])[1]\n",
        "        doc_outputs = self.doc_encoder(input_ids=doc_inputs['input_ids'], attention_mask=doc_inputs['attention_mask'])[1]\n",
        "        merged_outputs = torch.cat([query_outputs, doc_outputs], dim=-1)\n",
        "        merged_outputs = self.dropout(merged_outputs)\n",
        "        merged_outputs = self.fc1(merged_outputs)\n",
        "        merged_outputs = self.relu(merged_outputs)\n",
        "        merged_outputs = self.fc2(merged_outputs)\n",
        "        merged_outputs = self.softmax(merged_outputs)\n",
        "        return merged_outputs\n",
        "\n",
        "# Initialize the Two-Tower model and move it to the device (e.g., GPU)\n",
        "two_tower_model = TwoTowerModel().to(device)"
      ],
      "metadata": {
        "id": "75KHMCqT0xue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_texts = [...]  \n",
        "doc_texts = [...]  # list of strings containing the document text\n",
        "labels = [...]  # list of integers containing the labels (0 or 1)"
      ],
      "metadata": {
        "id": "mdPQJ_p-085J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_encodings = tokenizer(query_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "doc_encodings = tokenizer(doc_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(two_tower_model.parameters(), lr=2e-5)\n",
        "\n",
        "# Train the Two-Tower model on the training set\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i in range(len(query_encodings)):\n",
        "        query_inputs = {key: val[i].unsqueeze(0).to(device) for key, val in query_encodings.items()}\n",
        "        doc_inputs = {key: val[i].unsqueeze(0).to(device) for key, val in doc_encodings.items()}\n",
        "        label = labels[i].unsqueeze(0).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = two_tower_model(query_inputs, doc_inputs)\n",
        "        loss = criterion(outputs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(query_encodings)\n",
        "    print('Epoch %d loss: %.4f' % (epoch + 1, epoch_loss))"
      ],
      "metadata": {
        "id": "YVTpOntS1AKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Two-Tower model on the validation set\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i in range(len(val_query_encodings)):\n",
        "        query_inputs = {key: val[i].unsqueeze(0).to(device) for key, val in val_query_encodings.items()}\n",
        "        doc_inputs = {key: val[i].unsqueeze(0).to(device) for key, val in val_doc_encodings.items()}\n",
        "        label = val_labels[i].unsqueeze(0).to(device)\n",
        "\n",
        "        outputs = two_tower_model(query_inputs, doc_inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += 1\n",
        "        correct += (predicted == label).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print('Validation accuracy: %.2f%%' % accuracy)\n",
        "\n",
        "# Fine-tune the Two-Tower model on the validation set\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i in range(len(val_query_encodings)):\n",
        "        query_inputs = {key: val[i].unsqueeze(0).to(device) for key, val in val_query_encodings.items()}\n",
        "        doc_inputs = {key: val[i].unsqueeze(0).to(device) for key, val in val_doc_encodings.items()}\n",
        "        label = val_labels[i].unsqueeze(0).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = two_tower_model(query_inputs, doc_inputs)\n",
        "        loss = criterion(outputs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(val_query_encodings)\n",
        "    print('Epoch %d loss: %.4f' % (epoch + 1, epoch_loss))"
      ],
      "metadata": {
        "id": "YIWBOIah2BEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embeddings = []\n",
        "for i in range(len(doc_encodings)):\n",
        "    doc_inputs = {key: val[i].unsqueeze(0).to(device) for key, val in doc_encodings.items()}\n",
        "    doc_embeddings.append(two_tower_model.embedding_model(**doc_inputs)[0][:, 0, :].squeeze().detach().numpy())\n",
        "\n",
        "# Build the Annoy index for efficient nearest neighbor search\n",
        "num_trees = 10\n",
        "embedding_size = 128\n",
        "annoy_index = AnnoyIndex(embedding_size, metric='angular')\n",
        "for i in range(len(doc_embeddings)):\n",
        "    annoy_index.add_item(i, doc_embeddings[i])\n",
        "annoy_index.build(num_trees)\n",
        "\n",
        "# Perform nearest neighbor search for each test query\n",
        "k = 10  # number of nearest neighbors to retrieve\n",
        "for i in range(len(test_query_encodings)):\n",
        "    query_inputs = {key: val[i].unsqueeze(0).to(device) for key, val in test_query_encodings.items()}\n",
        "    query_embedding = two_tower_model.dense_layer(two_tower_model.embedding_model(**query_inputs)[0][:, 0, :]).squeeze().detach().numpy()\n",
        "    nearest_neighbors = annoy_index.get_nns_by_vector(query_embedding, k)\n",
        "    print('Query:', test_query_texts[i])\n",
        "    for neighbor in nearest_neighbors:\n",
        "        print('Document:', doc_texts[neighbor])"
      ],
      "metadata": {
        "id": "KqcCQq1t2J5I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}