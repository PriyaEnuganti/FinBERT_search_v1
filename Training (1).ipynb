{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-oSCBkDnKgU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Define the two-tower architecture\n",
        "class TwoTowerModel(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(TwoTowerModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "\n",
        "    def forward(self, query, doc):\n",
        "        query_output = self.base_model(**query)[0][:, 0, :]\n",
        "        doc_output = self.base_model(**doc)[0][:, 0, :]\n",
        "        return query_output, doc_output\n",
        "\n",
        "# Define the cosine similarity loss function\n",
        "class CosineSimilarityLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CosineSimilarityLoss, self).__init__()\n",
        "\n",
        "    def forward(self, query_output, doc_output, label):\n",
        "        query_output = query_output.unsqueeze(1)\n",
        "        doc_output = doc_output.unsqueeze(2)\n",
        "        similarity_matrix = nn.functional.cosine_similarity(query_output, doc_output, dim=-1)\n",
        "        loss = nn.functional.cross_entropy(similarity_matrix, label)\n",
        "        return loss\n",
        "\n",
        "# Load the pre-trained FinBERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "model = AutoModel.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "# Fine-tune the model on a relevant financial corpus\n",
        "# Replace the following with your own training code\n",
        "train_queries = []\n",
        "train_docs = []\n",
        "train_labels = []\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (query, doc, label) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        query_encodings = tokenizer(query, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "        doc_encodings = tokenizer(doc, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "        query_output, doc_output = model(**query_encodings, **doc_encodings)\n",
        "        loss = CosineSimilarityLoss()(query_output, doc_output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Save the model weights\n",
        "torch.save(model.state_dict(), \"finbert_two_tower.pt\")\n",
        "\n",
        "# Load the saved model weights\n",
        "model = AutoModel.from_pretrained(\"ProsusAI/finbert\")\n",
        "model_state_dict = torch.load(\"finbert_two_tower.pt\")\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "# Generate an embedding representation of the user query and compute the cosine similarity\n",
        "# Replace the following with your own inference code\n",
        "query = \"What is the price of Bitcoin today?\"\n",
        "doc_1 = \"Bitcoin rises to $60,000\"\n",
        "doc_2 = \"Ethereum falls to $2,000\"\n",
        "query_encodings = tokenizer(query, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "doc_1_encodings = tokenizer(doc_1, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "doc_2_encodings = tokenizer(doc_2, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "query_output, _ = model(**query_encodings, **doc_1_encodings)\n",
        "_, doc_1_output = model(**query_encodings, **doc_1_encodings)\n",
        "_, doc_2_output = model(**query_encodings, **doc_2_encodings)\n",
        "similarity_doc_1 = nn.functional.cosine_similarity(query_output, doc_1_output, dim=-1)\n",
        "similarity_doc_2 = nn.functional.cosine_similarity(query_output, doc_2_output, dim=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o24oNQGTnVnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "# Create Annoy index\n",
        "num_trees = 10\n",
        "embedding_size = query_output.shape[-1]\n",
        "index = AnnoyIndex(embedding_size, metric=\"angular\")\n",
        "for i in range(len(docs)):\n",
        "    doc_encodings = tokenizer(docs[i], truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    _, doc_output = model(**query_encodings, **doc_encodings)\n",
        "    doc_embedding = doc_output.detach().numpy()[0]\n",
        "    index.add_item(i, doc_embedding)\n",
        "index.build(num_trees)\n",
        "\n",
        "# Perform nearest neighbor search using the Annoy index\n",
        "k = 5\n",
        "query_encodings = tokenizer(query, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "_, query_output = model(**query_encodings, **doc_encodings)\n",
        "query_embedding = query_output.detach().numpy()[0]\n",
        "nn_indices = index.get_nns_by_vector(query_embedding, k)\n",
        "nn_docs = [docs[i] for i in nn_indices]"
      ],
      "metadata": {
        "id": "lSenJbM4nVw2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}