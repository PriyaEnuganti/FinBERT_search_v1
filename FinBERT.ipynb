{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "372GeJfG1fGt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "# Define the hyperparameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "embedding_size = 768\n",
        "num_trees = 10\n",
        "num_nearest_neighbors = 10\n",
        "\n",
        "# Load the data\n",
        "with open('data.txt', 'r') as f:\n",
        "    data = [line.strip().split('\\t') for line in f.readlines()]\n",
        "\n",
        "queries = [d[0] for d in data]\n",
        "documents = [d[1] for d in data]\n",
        "urls = [d[2] for d in data]\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "model = AutoModel.from_pretrained('ProsusAI/finbert')\n",
        "\n",
        "# Define the query tower\n",
        "class QueryTower(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QueryTower, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooler_output = outputs.pooler_output\n",
        "        return pooler_output\n",
        "\n",
        "# Define the document tower\n",
        "class DocumentTower(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DocumentTower, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooler_output = outputs.pooler_output\n",
        "        return pooler_output\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = torch.nn.CosineEmbeddingLoss()\n",
        "\n",
        "# Define the optimizer for the query tower\n",
        "optimizer_query = torch.optim.Adam(QueryTower().parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the query tower\n",
        "query_tower = QueryTower()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(len(queries)):\n",
        "        # Encode the query\n",
        "        query = tokenizer(queries[i], padding=True, truncation=True, return_tensors='pt')\n",
        "        # Forward pass\n",
        "        query_embedding = query_tower(query['input_ids'], query['attention_mask'])\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(query_embedding, torch.ones_like(query_embedding), torch.ones_like(query_embedding))\n",
        "        # Backward pass\n",
        "        optimizer_query.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_query.step()\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {loss.item():.4f}')\n",
        "\n",
        "# Define the optimizer for the document tower\n",
        "optimizer_document = torch.optim.Adam(DocumentTower().parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the document tower\n",
        "document_tower = DocumentTower()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(len(documents)):\n",
        "        # Encode the document\n",
        "        document = tokenizer(documents[i], padding=True, truncation=True, return_tensors='pt')\n",
        "        # Forward pass\n",
        "        document_embedding = document_tower(document['input_ids'], document['attention_mask'])\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(document_embedding, torch.zeros_like(document_embedding), torch.zeros_like(document_embedding))\n",
        "        # Backward pass\n",
        "        optimizer_document.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_document.step()\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {loss.item():.4f}')\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, key in enumerate(encoded_data.keys()):\n",
        "    index.add_item(i, encoded_data[key])\n",
        "\n",
        "# Build the Annoy index\n",
        "index.build(num_trees)\n",
        "\n",
        "# Save the Annoy index to disk\n",
        "index.save(index_file_path)\n",
        "\n",
        "# Query the Annoy index for the nearest neighbors\n",
        "for i, query in enumerate(queries):\n",
        "    # Encode the query\n",
        "    encoded_query = encode_text(query)\n",
        "    # Get the nearest neighbors\n",
        "    nearest_neighbors = index.get_nns_by_vector(encoded_query, num_nearest_neighbors)\n",
        "    # Print the results\n",
        "    print(f'Query {i}: {query}')\n",
        "    for neighbor in nearest_neighbors:\n",
        "        key = list(encoded_data.keys())[neighbor]\n",
        "        print(f'  Neighbor: {key}')\n",
        "        print(f'  Distance: {index.get_distance(i, neighbor)}')"
      ],
      "metadata": {
        "id": "cpR2OVZ_14eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(query, query_tower, document_tower, tokenizer, annoy_index, num_nearest_neighbors):\n",
        "    # Encode the query\n",
        "    query = tokenizer(query, padding=True, truncation=True, return_tensors='pt')\n",
        "    # Forward pass through the query tower\n",
        "    query_embedding = query_tower(query['input_ids'], query['attention_mask']).detach().numpy().squeeze()\n",
        "    # Find the nearest neighbors\n",
        "    nearest_neighbors = annoy_index.get_nns_by_vector(query_embedding, num_nearest_neighbors)\n",
        "    # Get the corresponding documents and URLs\n",
        "    documents = [data[i][1] for i in nearest_neighbors]\n",
        "    urls = [data[i][2] for i in nearest_neighbors]\n",
        "    return documents, urls"
      ],
      "metadata": {
        "id": "71cYyWFi4LOt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}