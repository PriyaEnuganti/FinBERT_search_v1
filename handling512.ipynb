{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCAZRx5_EJHy"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "model = AutoModel.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "# Example document with more than 512 tokens\n",
        "document = \"Long document with more than 512 tokens...\"\n",
        "\n",
        "# Split the document into chunks of 512 tokens\n",
        "max_length = 512\n",
        "document_chunks = [document[i:i+max_length] for i in range(0, len(document), max_length)]\n",
        "\n",
        "# Create embeddings for each chunk\n",
        "document_embeddings = []\n",
        "for chunk in document_chunks:\n",
        "    encoded_input = tokenizer(chunk, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "        embeddings = model_output.last_hidden_state[:, 0, :].numpy()\n",
        "        document_embeddings.append(embeddings)\n",
        "\n",
        "# Aggregate embeddings to create an overall embedding for the document\n",
        "document_embedding = np.mean(document_embeddings, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install annoy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I2PHv7BFdhF",
        "outputId": "b4e5f8df-d043-4882-c32b-3fc31eac1ec0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.2.tar.gz (647 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.4/647.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.2-cp310-cp310-linux_x86_64.whl size=582722 sha256=b9d9c1c73f290b594407d7784a56dd5ce63c0567400716262ae7b6038b749b31\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/d9/59/473fa56df8e39430eeda369500b4e7127f5b243ba24c3c4297\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModel.from_pretrained(\"ProsusAI/finbert\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "# Define the maximum length for document tokens\n",
        "max_length = 512\n",
        "\n",
        "# Define the number of trees for the Annoy index\n",
        "num_trees = 10\n",
        "\n",
        "# Define the dimensionality of the embeddings\n",
        "embedding_size = 768\n",
        "\n",
        "# Define the sliding window size and overlap\n",
        "window_size = 256\n",
        "overlap = 128\n",
        "\n",
        "# Define the list to store the document embeddings\n",
        "document_embeddings = []\n",
        "\n",
        "# Define a function to create document embeddings with sliding windows\n",
        "def create_document_embeddings(document):\n",
        "    # Tokenize the document\n",
        "    tokens = tokenizer.tokenize(document)\n",
        "    # Create sliding windows of the document\n",
        "    windows = [tokens[i:i+window_size] for i in range(0, len(tokens), window_size-overlap)]\n",
        "    # Create document embeddings for each window\n",
        "    embeddings = []\n",
        "    for window in windows:\n",
        "        # Encode the window with the tokenizer\n",
        "        inputs = tokenizer(\" \".join(window), max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        # Pass the inputs through the model to get the embeddings\n",
        "        with torch.no_grad():\n",
        "            output = model(**inputs)\n",
        "            embeddings.append(output.pooler_output.cpu().numpy()[0])\n",
        "    # Concatenate the embeddings to get the document embedding\n",
        "    document_embedding = np.concatenate(embeddings)\n",
        "    return document_embedding\n",
        "\n",
        "# Create the document embeddings\n",
        "documents = [\"your client can use the following funding tooltip\",\"schwab offers three main resources\"] # list of documents\n",
        "for document in documents:\n",
        "    document_embedding = create_document_embeddings(document)\n",
        "    document_embeddings.append(document_embedding)\n",
        "\n",
        "# Build the Annoy index\n",
        "annoy_index = AnnoyIndex(embedding_size, 'angular')\n",
        "for i in range(len(document_embeddings)):\n",
        "    annoy_index.add_item(i, document_embeddings[i])\n",
        "annoy_index.build(num_trees)\n",
        "\n",
        "# Define a function to perform semantic search\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKbdEie3FcNE",
        "outputId": "4c10b259-c017-4aab-b068-d5c3b473e799"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ProsusAI/finbert were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def semantic_search(query, k):\n",
        "    # Encode the query with the tokenizer\n",
        "    inputs = tokenizer(query, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
        "    # Pass the inputs through the model to get the query embedding\n",
        "    with torch.no_grad():\n",
        "        query_embedding = model(**inputs).pooler_output.cpu().numpy()[0]\n",
        "    # Use the Annoy index to retrieve the most similar documents\n",
        "    indices, distances = annoy_index.get_nns_by_vector(query_embedding, k, include_distances=True)\n",
        "    # Return the indices and distances\n",
        "    return indices, distances\n"
      ],
      "metadata": {
        "id": "R4FTOqVfS9FJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_search(\"cient fund account after opening\",2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rACzg22MSy-d",
        "outputId": "5c2e697c-5590-4feb-cc32-51efa200d9da"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 1], [0.31801414489746094, 0.37996208667755127])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z0cYttxwTNA2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}