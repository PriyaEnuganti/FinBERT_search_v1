!pip install keybert
!pip install nltk

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer, WordNetLemmatizer
from keybert import KeyBERT

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# Define the text to extract keywords from
text = """
Our company's revenue increased by 10% in the second quarter of 2022, driven by strong sales in our software division. Our gross profit margin improved by 2 percentage points due to cost reductions and operational efficiencies. However, our net income decreased by 5% due to higher R&D expenses and increased competition in the market. Despite these challenges, we remain optimistic about the future and are investing in new product development to drive growth in the coming quarters.
"""

# Define stop words, stemmer, and lemmatizer
stop_words = set(stopwords.words('english'))
stemmer = SnowballStemmer('english')
lemmatizer = WordNetLemmatizer()

# Tokenize the text
tokens = word_tokenize(text)

# Remove stop words, stem, and lemmatize the tokens
processed_tokens = []
for token in tokens:
    if token.lower() not in stop_words:
        stemmed_token = stemmer.stem(token)
        lemmatized_token = lemmatizer.lemmatize(stemmed_token)
        processed_tokens.append(lemmatized_token)

# Convert the processed tokens back to text
processed_text = ' '.join(processed_tokens)

# Load the pre-trained BERT model
model = KeyBERT('distilbert-base-nli-mean-tokens')

# Extract the top 10 keywords
keywords = model.extract_keywords(processed_text, keyphrase_ngram_range=(1, 3), use_maxsum=True, nr_candidates=20, top_n=10)

# Print the keywords
for keyword in keywords:
    print(keyword)
