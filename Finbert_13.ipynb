{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIysOleRxb6S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load FinBERT model and tokenizer\n",
        "model_name = \"ProsusAI/finbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "finbert = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Define query tower\n",
        "class QueryTower(nn.Module):\n",
        "    def __init__(self, finbert):\n",
        "        super(QueryTower, self).__init__()\n",
        "        self.finbert = finbert\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(768, 256)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.finbert(input_ids, attention_mask)\n",
        "        pooler_output = outputs[1]\n",
        "        pooler_output = self.dropout(pooler_output)\n",
        "        query_embedding = self.fc(pooler_output)\n",
        "        return query_embedding\n",
        "\n",
        "# Define document tower\n",
        "class DocumentTower(nn.Module):\n",
        "    def __init__(self, finbert):\n",
        "        super(DocumentTower, self).__init__()\n",
        "        self.finbert = finbert\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc1 = nn.Linear(768, 256)\n",
        "        self.fc2 = nn.Linear(5, 64)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, title, answer):\n",
        "        outputs = self.finbert(input_ids, attention_mask)\n",
        "        pooler_output = outputs[1]\n",
        "        pooler_output = self.dropout(pooler_output)\n",
        "        doc_embedding = self.fc1(pooler_output)\n",
        "        feat_embedding = self.fc2(torch.cat([title, answer], dim=1))\n",
        "        doc_embedding = torch.cat([doc_embedding, feat_embedding], dim=1)\n",
        "        return doc_embedding\n",
        "\n",
        "# Define two tower network\n",
        "class TwoTower(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TwoTower, self).__init__()\n",
        "        self.fc1 = nn.Linear(320, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, query_embedding, doc_embedding):\n",
        "        x = torch.cat([query_embedding, doc_embedding], dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model and send to device\n",
        "query_tower = QueryTower(finbert).to(device)\n",
        "doc_tower = DocumentTower(finbert).to(device)\n",
        "two_tower = TwoTower().to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(list(query_tower.parameters()) + list(doc_tower.parameters()) + list(two_tower.parameters()), lr=2e-5)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-xCJLgFQ0yC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ckkQDq2y0yFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# define model architecture\n",
        "class QueryTower(nn.Module):\n",
        "    def __init__(self, finbert_model):\n",
        "        super(QueryTower, self).__init__()\n",
        "        self.finbert_model = finbert_model\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, pooler_output = self.finbert_model(input_ids, attention_mask)\n",
        "        return pooler_output\n",
        "        \n",
        "class DocumentTower(nn.Module):\n",
        "    def __init__(self, finbert_model, num_features):\n",
        "        super(DocumentTower, self).__init__()\n",
        "        self.finbert_model = finbert_model\n",
        "        self.fc = nn.Linear(num_features, 64)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask, features):\n",
        "        _, pooler_output = self.finbert_model(input_ids, attention_mask)\n",
        "        features_output = self.fc(features)\n",
        "        concatenated_output = torch.cat((pooler_output, features_output), dim=1)\n",
        "        return concatenated_output\n",
        "        \n",
        "class TwoTower(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TwoTower, self).__init__()\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, query_output, document_output):\n",
        "        output = torch.cat((query_output, document_output), dim=1)\n",
        "        output = self.fc1(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        return output\n",
        "\n",
        "# prepare data\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "finbert_model = AutoModel.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "# sample data\n",
        "query = \"What is the capital of India?\"\n",
        "document = \"New Delhi is the capital of India.\"\n",
        "label = 1\n",
        "title = \"India's capital city\"\n",
        "answer = \"New Delhi\"\n",
        "\n",
        "# encode input\n",
        "query_input = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "document_input = tokenizer(document, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "features_input = torch.Tensor([title, answer])  # assume title and answer are preprocessed into numerical features\n",
        "\n",
        "# initialize model\n",
        "query_tower = QueryTower(finbert_model)\n",
        "document_tower = DocumentTower(finbert_model, num_features=2)\n",
        "two_tower = TwoTower()\n",
        "\n",
        "# define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(list(query_tower.parameters()) + list(document_tower.parameters()) + list(two_tower.parameters()), lr=0.001)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    query_output = query_tower(query_input[\"input_ids\"], query_input[\"attention_mask\"])\n",
        "    document_output = document_tower(document_input[\"input_ids\"], document_input[\"attention_mask\"], features_input)\n",
        "    similarity_score = two_tower(query_output, document_output)\n",
        "    loss = criterion(similarity_score, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "wy3KyWGF0yHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "# Load pre-trained query tower\n",
        "query_tower = load_query_tower()\n",
        "\n",
        "# Load document embeddings and labels\n",
        "document_embeddings = np.load('document_embeddings.npy')\n",
        "document_labels = np.load('document_labels.npy')\n",
        "\n",
        "# Index document embeddings using Annoy\n",
        "document_index = AnnoyIndex(embedding_dim, 'angular')\n",
        "for i, emb in enumerate(document_embeddings):\n",
        "    document_index.add_item(i, emb)\n",
        "document_index.build(50)\n",
        "\n",
        "# Function to perform semantic search\n",
        "def semantic_search(query, k=5):\n",
        "    # Get embedding for query using the query tower\n",
        "    query_embedding = query_tower(query)\n",
        "    \n",
        "    # Use Annoy index to retrieve most similar document embeddings\n",
        "    similar_doc_indices = document_index.get_nns_by_vector(query_embedding, k)\n",
        "    similar_doc_embeddings = document_embeddings[similar_doc_indices]\n",
        "    \n",
        "    # Calculate relevance scores for each retrieved document using two-tower network\n",
        "    relevance_scores = two_tower_net(query_embedding, similar_doc_embeddings)\n",
        "    \n",
        "    # Sort the documents based on relevance scores and return top-K\n",
        "    sorted_indices = np.argsort(relevance_scores)[::-1]\n",
        "    top_k_indices = sorted_indices[:k]\n",
        "    top_k_documents = document_labels[similar_doc_indices[top_k_indices]]\n",
        "    \n",
        "    return top_k_documents"
      ],
      "metadata": {
        "id": "A8WMpcoKN3LI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}