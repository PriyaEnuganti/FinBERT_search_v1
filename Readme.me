
import pandas as pd
import matplotlib.pyplot as plt

# Sample DataFrame
df = pd.DataFrame({'Business Name': ['Business 1', 'Business 2', 'Business 3', 'Business 4', 'Business 5', 'Business 6', 'Business 7', 'Business 8', 'Business 9']})

# Calculate the percentage of rows for each business name
business_counts = df['Business Name'].value_counts()
percentages = (business_counts / len(df)) * 100

# Identify business names with less than 2% occurrence
threshold = 2.0
below_threshold = percentages[percentages < threshold]

# Group below-threshold values into "Others"
below_threshold_sum = below_threshold.sum()
percentages = percentages[percentages >= threshold]
percentages['Others'] = below_threshold_sum

# Plot the pie chart
plt.pie(percentages, labels=percentages.index, autopct='%1.1f%%')

# Set aspect ratio to be equal so that the pie is drawn as a circle
plt.axis('equal')

# Set the title of the chart
plt.title('Percentage of Rows per Business Name')

# Display the plot
plt.show()




import pandas as pd
import matplotlib.pyplot as plt

# Sample DataFrame
df = pd.DataFrame({'Business Name': ['Business 1', 'Business 2', 'Business 3', 'Business 4', 'Business 5']})

# Calculate the percentage of rows for each business name
business_counts = df['Business Name'].value_counts()
percentages = (business_counts / len(df)) * 100

# Plot the pie chart
plt.pie(percentages, labels=business_counts.index, autopct='%1.1f%%')

# Set aspect ratio to be equal so that the pie is drawn as a circle
plt.axis('equal')

# Set the title of the chart
plt.title('Percentage of Rows per Business Name')

# Display the plot
plt.show()



https://www.scribd.com/document/510109067/iQMS-150-questions-and-answers
Data Extraction:

Identify the data sources we use in our CMS, such as databases, APIs, or external systems.
Determine the best method for extracting data from each source, like using SQL queries, RESTful API calls, or web scraping.
Set up scheduled jobs or triggers to fetch data from these sources on a regular basis or whenever new data becomes available.
Develop scripts or programs to automate the process of ingesting the extracted data into our search system.
Data Transformation:

Define rules and processes to transform and normalize the extracted data.
Create pipelines using tools like Apache Spark or Python scripts to apply the defined transformation rules.
Ensure the quality of the transformed data by implementing data validation and quality checks.
Indexing and Updating:

Choose the appropriate indexing mechanism for our search system, such as Elasticsearch or Solr.
Design the schema for indexing the data, mapping the fields from our CMS to the search index structure.
Automate the indexing process by developing scripts or jobs that trigger indexing whenever there are additions or modifications in the CMS.
Optimize the indexing process to handle incremental updates efficiently.
Query Processing:

Analyze user query patterns and behaviors to identify areas for optimization.
Develop automated rules or algorithms to rewrite or expand queries based on synonyms or stemming techniques.
Experiment with relevance tuning using techniques like A/B testing or machine learning.
Implement caching mechanisms to improve response times for frequently executed queries.
Monitoring and Alerts:

Define the key metrics we need to monitor, such as query latency and indexing throughput.
Set up monitoring tools like Prometheus or Grafana to collect and visualize the relevant metrics.
Establish alerting thresholds to receive notifications when certain metrics exceed predefined thresholds.
Configure notifications through email, Slack, or PagerDuty to stay informed about potential issues or performance degradation.
Search Analytics:

Collect search usage data, including query logs, click-through rates (CTRs), and user session information.
Analyze the data using tools like Apache Spark, ELK stack, or custom scripts to gain insights into search patterns and user behavior.
Generate automated reports or dashboards to present the search analytics data in a clear and actionable format.
Utilize the insights gained from the analytics to make improvements in search relevance and user experience.











df = pd.read_csv('your_dataset.csv')
grouped = df.groupby('category')['keywords'].apply(lambda x: ' '.join(x)).reset_index()
def generate_wordcloud(text):
    stopwords = set(STOPWORDS)
    wordcloud = WordCloud(width=800, height=800,
                          background_color='white',
                          stopwords=stopwords,
                          min_font_size=10).generate(text)
    return wordcloud
    
for index, row in grouped.iterrows():
    category = row['category']
    text = row['keywords']
    wordcloud = generate_wordcloud(text)
    plt.figure(figsize=(8, 8), facecolor=None)
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.tight_layout(pad=0)
    plt.title(category)
    plt.show()




















import requests
from bs4 import BeautifulSoup

# Login credentials
username = 'your_username'
password = 'your_password'

# Login URL and search URL
login_url = 'https://example.com/login'
search_url = 'https://example.com/search'

# Create a session to persist the login cookies
session = requests.session()

# Send a POST request with login credentials to the login URL
login_data = {
    'username': username,
    'password': password
}
response = session.post(login_url, data=login_data)

# Check if login was successful
if response.status_code == 200:
    # Send a GET request to the search URL
    search_term = 'your_search_term'
    search_params = {
        'q': search_term
    }
    response = session.get(search_url, params=search_params)

    # Parse the HTML response using BeautifulSoup
    soup = BeautifulSoup(response.content, 'html.parser')

    # Process the search results as required
    # ...
else:
    print('Login failed')





import requests
from bs4 import BeautifulSoup

search_term = 'Python programming'

# Send a GET request to the Google search page
url = f'https://www.google.com/search?q={search_term}'
response = requests.get(url)

# Parse the HTML response using BeautifulSoup
soup = BeautifulSoup(response.content, 'html.parser')

# Find all the search results on the page
search_results = soup.find_all('div', class_='g')

# Print the title and URL of each search result
for result in search_results:
    title = result.find('h3').text
    url = result.find('a')['href']
    print(f'{title}: {url}')




Preprocessing the data: The first step is to preprocess the data. The documents and user queries need to be cleaned and tokenized before feeding them into the model. You may also need to apply some specific preprocessing steps depending on the nature of the data.


Training all three components (query tower, document tower, and two-tower network) in the same loop is known as end-to-end training. End-to-end training is often used in deep learning when all components of the system are jointly optimized to minimize a loss function.

The benefit of end-to-end training is that the entire system can be optimized jointly, allowing for more efficient use of the data and potentially better overall performance. In the context of semantic search, end-to-end training allows the query tower, document tower, and two-tower network to be optimized together to minimize the loss between the predicted and ground truth relevance scores for the query-document pairs.






FinBERT model: Load the pre-trained FinBERT model and fine-tune it on a relevant financial corpus. This is done by feeding the financial documents into the model and allowing it to update its parameters based on the financial vocabulary and syntax. This step will ensure that the embeddings generated by the model are tailored for the financial domain.

Two-tower architecture: The two-tower architecture consists of two identical models with shared weights. Each tower represents a document or query respectively. In this case, we will use the same FinBERT model for both towers. The model will take in the input text and output the embedding representation of that text.

Loss function: Once the embeddings have been generated, a loss function is used to compare the similarity of the embeddings between the query and documents. The cosine similarity is commonly used as the loss function in this type of architecture.

Training: The model is then trained on a dataset of queries and relevant documents. During training, the model learns to generate embeddings that are similar for relevant documents and queries and dissimilar for irrelevant documents and queries.

Inference: Once the model has been trained, it can be used for inference. Given a user query, the model generates an embedding representation of the query. The model then computes the cosine similarity between the query embedding and the embedding representations of all the documents in the corpus. The documents with the highest cosine similarity scores are returned as the top matches.

Overall, this approach allows you to leverage the power of FinBERT for financial domain-specific document retrieval while also tailoring the embeddings for your specific use case using the two-tower architecture.



for i, (document, url) in enumerate(documents):
    embedding = model.encode(document)
    index.add_item(i, embedding)
    index.set_item_vector(i, embedding)
    index.set_item_data(i, json.dumps({"url": url}))

# Build the index
index.build(10)

# Query the index and retrieve the URLs of the top results
query = "What is the price of AAPL stock?"
query_embedding = model.encode(query)
results = index.get_nns_by_vector(query_embedding, 5)
for i in results:
    item_data = json.loads(index.get_item_data(i))
    print(item_data["url"])






Gather a dataset of documents and their corresponding queries. This dataset should also contain relevance labels indicating whether a given document is relevant or not for a given query.

Preprocess the dataset by tokenizing the queries and documents using the same tokenization method used to pretrain the FinBERT model. You should also encode the queries and documents using the FinBERT tokenizer and create a numerical representation for each.

Divide the dataset into training and validation sets.

Define the two-tower architecture. In this architecture, you will use two identical FinBERT models to represent the queries and documents. Each FinBERT model will have its own set of learned weights. During training, the weights of both models will be updated simultaneously. The output of each FinBERT model will be a vector representation of the query or document.

Define the loss function. The loss function will compare the similarity between the query and the relevant documents and the dissimilarity between the query and the irrelevant documents. The similarity can be measured using the cosine similarity between the query vector and the document vector.

Train the two-tower model on the training set. During training, the model will learn to map the query vectors and document vectors into a common space where relevant documents are close to the corresponding query vector.

Evaluate the model on the validation set. You can use metrics such as Mean Reciprocal Rank (MRR) or Normalized Discounted Cumulative Gain (NDCG) to evaluate the performance of the model.

If the performance of the model is not satisfactory, you can adjust the hyperparameters or the architecture of the model and repeat the training process until you achieve the desired performance.

Once you are satisfied with the performance of the model, you can use it to retrieve relevant documents for new queries.
