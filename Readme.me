
query_input_ids = torch.cat((query_embedding['input_ids'], torch.zeros(self.max_length - len(query_embedding['input_ids']), dtype=torch.long)), dim=0)


class EmbeddingDataset(Dataset):
    def __init__(self, query_embeddings, doc_embeddings, labels):
        self.query_embeddings = query_embeddings
        self.doc_embeddings = doc_embeddings
        self.labels = labels

        # Determine the maximum length of the embeddings
        self.max_length = max([len(embedding['input_ids']) for embedding in query_embeddings + doc_embeddings])
        
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        query_embedding = self.query_embeddings[idx]
        doc_embedding = self.doc_embeddings[idx]
        label = self.labels[idx]

        # Pad the query and document embeddings
        query_input_ids = query_embedding['input_ids'] + [0] * (self.max_length - len(query_embedding['input_ids']))
        query_attention_mask = query_embedding['attention_mask'] + [0] * (self.max_length - len(query_embedding['attention_mask']))
        doc_input_ids = doc_embedding['input_ids'] + [0] * (self.max_length - len(doc_embedding['input_ids']))
        doc_attention_mask = doc_embedding['attention_mask'] + [0] * (self.max_length - len(doc_embedding['attention_mask']))
        
        return {'query_input_ids': query_input_ids,
                'query_attention_mask': query_attention_mask,
                'doc_input_ids': doc_input_ids,
                'doc_attention_mask': doc_attention_mask,
                'label': label}





query_embedding = pad_sequence([torch.tensor(embedding) for embedding in query_embedding], 
                                        batch_first=True, padding_value=0.0)
doc_embedding = pad_sequence([torch.tensor(embedding) for embedding in doc_embedding], 
                                        batch_first=True, padding_value=0.0)
        



import torch
from torch.utils.data import Dataset, DataLoader

class MyDataset(Dataset):
    def __init__(self, query_embeddings, doc_embeddings, labels):
        self.query_embeddings = query_embeddings
        self.doc_embeddings = doc_embeddings
        self.labels = labels
        
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        query_embedding = self.query_embeddings[idx]
        doc_embedding = self.doc_embeddings[idx]
        label = self.labels[idx]
        
        return query_embedding, doc_embedding, label

batch_size = 12
dataset = MyDataset(query_embeddings, doc_embeddings, labels)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
In this case, the __getitem__ method returns the query embedding, document embedding, and label as they are without converting them into tensors.












max_length = max([len(embedding) for embedding in embeddings])
    padded_embeddings = []
    for embedding in embeddings:
        if len(embedding) < max_length:
            padding_length = max_length - len(embedding)
            embedding = np.pad(embedding, [(0, padding_length)], mode='constant')
        padded_embeddings.append(embedding)


last_hidden_state = outputs.last_hidden_state
        pooled_output = last_hidden_state[:, 0, :]
        pooled_output = self.dropout(pooled_output)
        refined_query_embedding = self.linear(pooled_output)
        return refined_query_embedding

Preprocessing the data: The first step is to preprocess the data. The documents and user queries need to be cleaned and tokenized before feeding them into the model. You may also need to apply some specific preprocessing steps depending on the nature of the data.


Training all three components (query tower, document tower, and two-tower network) in the same loop is known as end-to-end training. End-to-end training is often used in deep learning when all components of the system are jointly optimized to minimize a loss function.

The benefit of end-to-end training is that the entire system can be optimized jointly, allowing for more efficient use of the data and potentially better overall performance. In the context of semantic search, end-to-end training allows the query tower, document tower, and two-tower network to be optimized together to minimize the loss between the predicted and ground truth relevance scores for the query-document pairs.






FinBERT model: Load the pre-trained FinBERT model and fine-tune it on a relevant financial corpus. This is done by feeding the financial documents into the model and allowing it to update its parameters based on the financial vocabulary and syntax. This step will ensure that the embeddings generated by the model are tailored for the financial domain.

Two-tower architecture: The two-tower architecture consists of two identical models with shared weights. Each tower represents a document or query respectively. In this case, we will use the same FinBERT model for both towers. The model will take in the input text and output the embedding representation of that text.

Loss function: Once the embeddings have been generated, a loss function is used to compare the similarity of the embeddings between the query and documents. The cosine similarity is commonly used as the loss function in this type of architecture.

Training: The model is then trained on a dataset of queries and relevant documents. During training, the model learns to generate embeddings that are similar for relevant documents and queries and dissimilar for irrelevant documents and queries.

Inference: Once the model has been trained, it can be used for inference. Given a user query, the model generates an embedding representation of the query. The model then computes the cosine similarity between the query embedding and the embedding representations of all the documents in the corpus. The documents with the highest cosine similarity scores are returned as the top matches.

Overall, this approach allows you to leverage the power of FinBERT for financial domain-specific document retrieval while also tailoring the embeddings for your specific use case using the two-tower architecture.



for i, (document, url) in enumerate(documents):
    embedding = model.encode(document)
    index.add_item(i, embedding)
    index.set_item_vector(i, embedding)
    index.set_item_data(i, json.dumps({"url": url}))

# Build the index
index.build(10)

# Query the index and retrieve the URLs of the top results
query = "What is the price of AAPL stock?"
query_embedding = model.encode(query)
results = index.get_nns_by_vector(query_embedding, 5)
for i in results:
    item_data = json.loads(index.get_item_data(i))
    print(item_data["url"])






Gather a dataset of documents and their corresponding queries. This dataset should also contain relevance labels indicating whether a given document is relevant or not for a given query.

Preprocess the dataset by tokenizing the queries and documents using the same tokenization method used to pretrain the FinBERT model. You should also encode the queries and documents using the FinBERT tokenizer and create a numerical representation for each.

Divide the dataset into training and validation sets.

Define the two-tower architecture. In this architecture, you will use two identical FinBERT models to represent the queries and documents. Each FinBERT model will have its own set of learned weights. During training, the weights of both models will be updated simultaneously. The output of each FinBERT model will be a vector representation of the query or document.

Define the loss function. The loss function will compare the similarity between the query and the relevant documents and the dissimilarity between the query and the irrelevant documents. The similarity can be measured using the cosine similarity between the query vector and the document vector.

Train the two-tower model on the training set. During training, the model will learn to map the query vectors and document vectors into a common space where relevant documents are close to the corresponding query vector.

Evaluate the model on the validation set. You can use metrics such as Mean Reciprocal Rank (MRR) or Normalized Discounted Cumulative Gain (NDCG) to evaluate the performance of the model.

If the performance of the model is not satisfactory, you can adjust the hyperparameters or the architecture of the model and repeat the training process until you achieve the desired performance.

Once you are satisfied with the performance of the model, you can use it to retrieve relevant documents for new queries.
